% ==========================================
% BAB I PENDAHULUAN
% ==========================================
\chapter{PENDAHULUAN}
\label{chap:pendahuluan}
% --- Latar Belakang ---
\section{Latar Belakang}
Perkembangan teknologi \textit{Artificial Intelligence} menimbulkan cara baru untuk menciptakan berita hoaks melalui \textit{deepfake}. \textit{Deepfake} adalah teknik untuk menempatkan gambar wajah orang ``sebenarnya'' dalam suatu video menjadi wajah target sehingga seolah-olah target tersebut melakukan atau mengatakan hal-hal yang dilakukan orang ``sebenarnya''. Menurut laporan dari Sensity AI, terjadi lonjakan kasus \textit{deepfake} sebesar 550\% sejak tahun 2019 \autocite{komdigi2025}, didapatkan bahwa 90\% di antaranya digunakan untuk menyebarkan disinformasi, melakukan penipuan, pencemaran nama baik, hingga pelecehan yang menargetkan kelompok rentan seperti perempuan dan anak-anak. Di Indonesia sendiri, \textit{deepfake} sering kali digunakan untuk memprovokasi sentimen negatif dari publik. Fenomena ini menunjukkan urgensi dari pengembangan teknologi untuk dapat mendeteksi \textit{deepfake} secara efektif.

Salah satu tantangan dalam pendeteksian \textit{deepfake} adalah ketika suatu video \textit{deepfake} muncul, video tersebut akan diunggah dan disebarkan melalui berbagai \textit{platform} media sosial. Proses kompresi yang dilakukan pada media sosial seperti Youtube, Tiktok, WhatsApp, dan lain sebagainya akan menyebabkan kualitas video tersebut menurun. Kualitas video \textit{deepfake} yang rendah akan semakin sulit untuk dideteksi, baik oleh manusia maupun oleh teknologi AI. Akibatnya, banyak model deteksi yang menunjukkan akurasi tinggi di lingkungan laboratorium (mencapai hampir 100\%) mengalami penurunan kinerja drastis hingga hanya sekitar 65\% saat dihadapkan pada skenario dunia nyata dengan video terkompresi \autocite{alrashoud2025}. Masalah ini menjadi semakin relevan di Indonesia, terdapat disparitas kecepatan internet seluler yang signifikan antar regional. Studi menunjukkan bahwa Jawa memiliki kecepatan internet tertinggi sementara Papua menunjukkan kecepatan terendah, yang dipengaruhi oleh infrastruktur telekomunikasi yang tidak merata dan kondisi geografis \autocite{anugrah2025}. Kualitas konektivitas yang rendah ini seringkali memaksa kompresi video yang lebih agresif agar konten dapat diakses dan dibagikan, yang secara langsung memperparah kesulitan dalam mendeteksi artefak deepfake pada video berkualitas rendah.

Berbagai pendekatan deteksi canggih sebenarnya telah dikembangkan untuk mengatasi manipulasi ini. Solusi tersebut meliputi analisis artefak spasial pada wajah menggunakan \textit{Convolutional Neural Networks} (CNNs) \autocite{li2019}, analisis inkonsistensi temporal antar-frame seperti kedipan mata yang tidak natural menggunakan \textit{Recurrent Neural Networks} (RNNs) \autocites{fernandes2025}{alrashoud2025}, hingga metode deteksi sinyal biologis seperti detak jantung (rPPG) yang sulit ditiru generator \textit{deepfake} \autocite{ciftci2020}. Namun, efektivitas seluruh metode ini sangat bergantung pada kualitas data yang digunakan untuk melatihnya. Metode-metode tersebut sering kali gagal ketika diuji pada kondisi \textit{out-of-distribution}, yaitu ketika data uji (video media sosial terkompresi) memiliki karakteristik yang jauh berbeda dari data latih (video berkualitas tinggi). Tanpa adanya data yang merepresentasikan degradasi kualitas ini, algoritma secanggih apa pun tidak akan optimal dalam skenario dunia nyata.

Keterbatasan ini menyoroti pergeseran paradigma penting dalam pengembangan AI saat ini, yaitu dari pendekatan \textit{model-centric} (fokus pada perbaikan arsitektur) menuju pendekatan \textit{data-centric} (fokus pada kualitas data). Menurut \autocite{zha2023}, peningkatan kualitas dan representasi data sering kali memberikan dampak kinerja yang lebih signifikan dibanding sekadar memodifikasi model. Dalam konteks ini, \textit{dataset engineering} bukan lagi sekadar pengumpulan data pasif, melainkan proses desain sistematis untuk memastikan cakupan fenomena sains terpenuhi \autocite{vitalbrazil2023}. Oleh karena itu, \textit{bottleneck} deteksi deepfake di Indonesia saat ini bukanlah ketiadaan model deteksi, melainkan ketiadaan \textit{dataset} yang representatifâ€”baik dari segi demografi wajah Indonesia maupun karakteristik kompresi jaringan lokal.

Kesenjangan ketersediaan data ini terlihat jelas pada \textit{dataset benchmark} yang ada. \textit{Dataset} populer seperti yang dirangkum oleh \autocite{sohan2023}, mayoritas berisi video resolusi tinggi dengan subjek wajah non-Indonesia, sehingga validitasnya dipertanyakan untuk mendeteksi konten lokal \autocite{banerjee2024}{graupner2025}. Penelitian ini diusulkan untuk mengisi kekosongan tersebut dengan mengembangkan \textit{dataset} baru yang secara spesifik dirancang untuk mencakup dua tantangan utama yaitu konteks demografis Indonesia dan simulasi artefak kompresi video kualitas rendah yang realistis.\

Penelitian ini didasarkan pada analisis mendalam terhadap metodologi yang digunakan oleh \textit{benchmark} global, keberhasilan mereka terletak pada kontrol penuh atas \textit{pipeline} data. Misalnya, FaceForensics++ yang merupakan salah satu dataset paling masif yang dibangun secara sistematis melalui tiga langkah terkontrol yaitu Pengumpulan (dengan kriteria video resolusi tinggi), Manipulasi (menggunakan empat teknik \textit{swapping} dan \textit{reenactment} terkemuka), dan Kompresi (\textit{Post-processing}) yang menghasilkan tiga tingkat kualitas video (\textit{Raw}, \textit{High-Quality}, \textit{Low-Quality}) untuk evaluasi yang terukur. Pendekatan ini adalah contoh sempurna dari metodologi \textit{data-centric} yang menjamin \textit{ground truth} dan memungkinkan \textit{slicing} data berdasarkan artefak. Namun, seperti banyak \textit{dataset} terdahulu lainnya, FaceForensics++ dan Celeb-DFv2 menggunakan subjek wajah non-Indonesia dan tidak secara akurat mereplikasi kompresi agresif dari media sosial yang relevan dengan konteks lokal. Oleh karena itu, penelitian ini mengadopsi struktur metodologis \textit{end-to-end} yang sama yaitu mengontrol Pengumpulan, Generasi, dan Simulasi Kompresi. Namun, menerapkan hal tersebut pada konten lokal Indonesia dan teknik simulasi kompresi yang lebih realistis, untuk mengatasi kesenjangan \textit{Out-of-Distribution} yang disorot oleh \autocite{montibeller2025}. Pendekatan ini memastikan \textit{dataset} yang dihasilkan valid secara ilmiah dan relevan dengan ancaman disinformasi di Indonesia.

% --- Rumusan Masalah ---
\section{Rumusan Masalah}
Masalah utama yang ingin diselesaikan dalam tugas akhir ini adalah belum adanya model deteksi \textit{deepfake} yang andal dan teruji untuk menangani video berkualitas rendah dalam konteks Indonesia. Mayoritas solusi yang ada saat ini dilatih menggunakan \textit{dataset} video resolusi tinggi dengan subjek non-Indonesia, sehingga performanya menurun drastis saat dihadapkan pada video terkompresi dan gagal mengenali karakteristik demografis lokal. Jika tidak diatasi, masyarakat akan semakin rentan terhadap ancaman disinformasi dan penipuan berbasis \textit{deepfake}.Dalam menjawab permasalahan tersebut, penelitian ini akan berfokus pada pembuatan dataset yang representatif dan pengembangan model deteksi yang robust. Berikut adalah beberapa rumusan masalah spesifik yang akan diselesaikan
\begin{enumerate}
\item	Bagaimana metodologi yang efektif untuk mengumpulkan dan menyeleksi video asli berkualitas tinggi yang beragam dari sumber lokal Indonesia sebagai bahan dasar pembuatan \textit{dataset}, dengan memperhatikan keragaman fenotipe (visual) dan logat (audio) subjek?
\item	Apa saja jenis-jenis manipulasi \textit(deepfake) (visual, audio, dan multimodal) yang perlu diterapkan pada video asli agar hasilnya relevan dengan ancaman disinformasi di Indonesia, dan bagaimana proses penerapannya dapat dikontrol untuk menjamin \textit{ground truth}?
\item Bagaimana cara mensimulasikan efek kompresi media sosial (\textit{lossy compression}) secara sistematis, terukur, dan realistis untuk menghasilkan video berkualitas rendah yang mengandung artefak forensik khas, agar model deteksi dapat bergeneralisasi di skenario dunia nyata?
\item Bagaimana cara melakukan validasi internal dan eksternal terhadap \textit{dataset} yang akan dibuat untuk menjamin validitas label 100\% dan mencapai keseimbangan yang wajar antara kelas video (asli/palsu) dan kualitas video (tinggi/rendah)? 
\end{enumerate}

% --- Tujuan ---
\section{Tujuan}
Penelitian ini bertujuan untuk mengembangkan \textit{dataset} dalam konteks Indonesia guna mendukung deteksi video \textit{deepfake}. \textit{Dataset} disusun melalui pembuatan sampel video \textit{deepfake} yang mereplikasi karakteristik dan kualitas visual video yang beredar di media sosial.

% --- Batasan Masalah ---
\section{Batasan Masalah}
Berikut merupakan beberapa batasan yang ditetapkan untuk memfokuskan ruang lingkup dan memastikan hasil dari tugas akhir ini relevan dengan tujuan yang ditetapkan.
\begin{enumerate}
  \item Tugas akhir ini dikerjakan secara berkelompok yang terdiri dari 3 orang mahasiswa, yaitu Alvin Fadhilah Akmal (NIM 18222079), Harry Truman Suhalim (NIM 18222081), dan Steven Adrian Corne (NIM 18222101). Adapun pembagian fokus dari ketiga mahasiswa tersebut ialah Harry fokus pada \textit{dataset} \textit{deepfake}, Steven fokus pada model untuk mendeteksi \textit{deepfake}, dan Alvin fokus pada \textit{browser extension}.
  \item Penelitian akan menggunakan \textit{dataset} video yang dirancang untuk merepresentasikan konteks Indonesia. Ini mencakup pengumpulan video asli yang menampilkan wajah orang Indonesia dan pembuatan video deepfake dari bahan tersebut.
  \item \textit{Dataset} ini dikembangkan hanya untuk kepentingan pendidikan, penelitian, dan penulisan karya ilmiah. Video asli dikumpulkan berdasarkan prinsip \textit{Fair Use} dan Pasal 43 (a) UU RI No. 28 Tahun 2014, sehingga penggunaannya non-komersial dan tidak merugikan kepentingan Pencipta secara wajar.
  \item Proses generasi \textit{deepfake} dilakukan secara internal dan terkontrol hanya untuk tujuan penelitian. \textit{Dataset} tidak akan memasukkan \textit{deepfake} dengan konten sensitif atau yang menargetkan individu secara berbahaya (seperti pelecehan atau penipuan) untuk menghindari potensi penyalahgunaan.
  \item \textit{Dataset} yang dikembangkan dalam penelitian ini secara khusus berfokus pada konteks lokal Indonesia. Populasi subjek yang digunakan dalam \textit{dataset} ini dibatasi pada perwakilan dari 13 kelompok suku bangsa utama di Indonesia untuk menjamin representasi visual dan audio yang relevan dengan keragaman lokal. \textit{Dataset} ini ditujukan untuk skenario media sosial dan video berkualitas rendah, sehingga batasan kualitas video adalah pada resolusi dan simulasi kompresi yang meniru \textit{platform in-the-wild}.
  \item Pengembangan \textit{dataset} dilakukan dengan mematuhi standar etika dan regulasi yang berlaku (UU Perlindungan Data Pribadi), di mana sumber data dibatasi pada video domain publik (\textit{YouTube}) sesuai prinsip \textit{Fair Use}, proses manipulasi dilakukan secara terkontrol hanya untuk tujuan penelitian defensif, serta distribusi \textit{dataset} yang dibatasi untuk kalangan terbatas guna mencegah penyalahgunaan.
\end{enumerate}

% --- Metodologi Pengerjaan TA ---
\section{Metodologi}
Tahapan yang akan dilalui selama pelaksanaan tugas akhir ini terdiri dari tiga bagian, yaitu:
\begin{enumerate}
  \item Perumusan masalah awal \\
  Pada tahapan ini, dilakukan identifikasi masalah awal dengan cara \textit{brainstorming} bersama dengan seluruh anggota kelompok tugas akhir untuk menentukan masalah yang akan diangkat menjadi topik tugas akhir. Bermula dari pengamatan salah seorang anggota kelompok, terkait sulit membedakan berita mana yang hoax dan berita mana yang asli. Seluruh anggota kelompok mengalami hal yang sama, dikarenakan berita hoax terlalu luas maka kami menyepakati untuk mendeteksi hoax berupa \textit{deepfake}. 
  Untuk mempelajari tentang \textit{deepfake} terutama dalam konteks hoax kami melakukan riset lebih lanjut. Akhirnya ditemukan data dan fakta bahwa \textit{deepfake} saat ini sering kali digunakan untuk menyebarkan hoax dan ditemukan juga beberapa jurnal yang membahas \textit{deepfake}. Data dan informasi yang didapat dari jurnal tersebut akhirnya memperkuat masalah yang ingin kami selesaikan dikarenakan dalam beberapa jurnal disebutkan bahwa model deteksi \textit{deepfake} yang mereka miliki belum dapat mendeteksi video dengan kualitas rendah dan juga model kurang relevan digunakan apabila konteks \textit{deepfake} di Indonesia.
  \item Pencarian dan Penulisan Studi Literatur \\
  Penentuan teori-teori yang dirasa perlu dalam penulisan tugas akhir ini dilakukan dengan mengidentifikasikan hal-hal seputar fokus yang dibahas oleh setiap anggota kelompok tugas akhir. Tugas akhir ini fokus pada dataset sehingga teori-teori yang dibutuhkan adalah teori seputar metodologi pembuatan \textit{dataset deepfake} publik, teknik generasi \textit{deepfake}, serta dampak dan simulasi dari kompresi video. 
  Metodologi pembuatan \textit{dataset} sendiri dipilih dengan tujuan untuk menyamakan \textit{dataset} dengan konten lokal yang ada di Indonesia sehingga lebih relevan. Selain itu, dapat juga disesuaikan kualitas dari video untuk menyesuaikan dengan kondisi video disebar luaskan di sosial media.
  Adapun penelusuran pustaka lebih dalam terkait setiap teori yang diperlukan dilakukan melalui internet. Untuk setiap teori, dilakukan pencarian terhadap jurnal-jurnal yang membahas spesifik terkait teori tersebut. Kata kunci pencarian yang digunakan antara lain \textit{"deepfake dataset creation", "face forensics", "video compression simulation", dan "generative adversarial networks for face synthesis"}. Untuk setiap area, proses dimulai dengan mencari survey paper guna mendapatkan gambaran umum, lalu dilanjutkan dengan penelusuran mendalam terhadap artikel-artikel kunci yang relevan untuk dipelajari metodologinya secara detail.
  \item Pengembangan \textit{Dataset} \\
  Pengembangan \textit{dataset} dibuat secara manual dengan membuat deepfake sendiri maupun mengambil deepfake yang ada di sosial media. Metodologi dalam pengembangan dataset dibagi menjadi empat fase yaitu:
  \begin{enumerate}[label=\alph*.]
    \item Fase Pengumpulan (\textit{Collection}) \\
    Pada fase ini, dilakukan pengumpulan bahan mentah untuk \textit{dataset}. Fokus utama adalah mendapatkan data video asli yang merepresentasikan konten lokal Indonesia. Kebutuhan awal, seperti jumlah video, durasi, dan variasi subjek, ditetapkan untuk memastikan \textit{dataset} yang dihasilkan cukup beragam. Video terkait tokoh publik, \textit{vlogger}, dan individu lainnya pada sosial media diunduh dengan kualitas tinggi, video \textit{deepfake} yang tersebar pada sosial media juga dikumpulkan.
    \item Fase Generasi dan Pemrosesan (\textit{Construction})
    Setelah bahan mentah terkumpul, dilakukan proses pembuatan data \textit{deepfake}.  Fase ini diawali dengan penggunaan video asli untuk menghasilkan video manipulasi menggunakan beberapa teknik, seperti \textit{face swapping} dan \textit{lip-sync}, dengan perangkat lunak open-source untuk menciptakan variasi artefak.
    Setelah itu, dilakukan simulasi kompresi menggunakan \textit{tools} seperti FFmpeg untuk menurunkan kualitas video secara terkontrol, sehingga menghasilkan versi berkualitas rendah yang realistis.
    \item Fase Pelabelan (\textit{Labeling}) \\
    Pada fase ini, setiap data yang telah diproses akan diberi label informatif untuk memberikan konteks bagi model \textit{machine learning}. Setiap video akan diklasifikasikan dengan label utama (asli atau palsu). Selanjutnya, sebuah file metadata akan dibuat untuk mencatat informasi yang lebih detail untuk setiap data, seperti ID video, label utama, jenis manipulasi yang digunakan (jika palsu), dan tingkat kompresi yang diterapkan.
    \item Fase Validasi (\textit{Transition}) \\
    Setelah pelabelan selesai, \textit{dataset} divalidasi untuk memastikan kualitas dan integritasnya. Dilakukan pemeriksaan silang secara acak terhadap sampel data untuk memastikan pelabelan sudah akurat. Selain itu, diperiksa keseimbangan antara kelas asli dan palsu serta antara data berkualitas tinggi dan rendah. Tujuannya adalah digunakan untuk melakukan perbaikan minor pada \textit{dataset} sebelum digunakan untuk pelatihan model. Validasi teknis dilakukan melalui pengujian performa model, di mana akurasi model yang dilatih menggunakan \textit{dataset} ini dibandingkan secara komparatif dengan model yang dilatih pada \textit{dataset benchmark} untuk mengukur efektivitas dan reliabilitas \textit{dataset} yang dikembangkan.
  \end{enumerate}
\end{enumerate}